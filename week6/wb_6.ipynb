{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "\n",
    "# ― NER in action with Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Flair?\n",
    "\n",
    "There are several arguments:\n",
    "\n",
    "+ It builds upon PyTorch, which excels on many dimensions\n",
    "+ It's flexible ― users can choose among several models\n",
    "+ It draws upon novel, context-aware models (BERT & friends)  \n",
    "+ It has its own embeddings\n",
    "+ In relative terms, it's very accurate\n",
    "\n",
    "<img src='images/_22.png' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair's competitors\n",
    "\n",
    "<img src='images/_21.png' width=50%>\n",
    "\n",
    "Source: https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy's prodigy library\n",
    "\n",
    "<img src='images/_23.png' width=50%>\n",
    "\n",
    "Source: https://prodi.gy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of Flair\n",
    "\n",
    "Flair depends on a fair number of complex libraries; here are some tips to\n",
    "to succesfully go through the installation process:\n",
    "\n",
    "+ create an ad-hoc environment\n",
    "+ Python 3.6/3.7 are preferrable\n",
    "+ Install PyTorch (torch) first\n",
    "+ `pip install flair` (even if you're Conda user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Flair + some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair.datasets\n",
    "from flair.models import SequenceTagger\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.embeddings import FlairEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-24 13:46:56,798 loading file /home/simone/.flair/models/en-ner-conll03-v0.4.pt\n",
      "2020-06-24 13:46:57,852 loading file /home/simone/.flair/models/en-frame-ontonotes-v0.4.pt\n",
      "2020-06-24 13:47:00,731 Reading data from /home/simone/.flair/datasets/ud_english\n",
      "2020-06-24 13:47:00,731 Train: /home/simone/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-06-24 13:47:00,732 Dev: /home/simone/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2020-06-24 13:47:00,733 Test: /home/simone/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('ner')\n",
    "framing = SequenceTagger.load('frame')\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "corpus = flair.datasets.UD_ENGLISH()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences are the fundamental unit of analysis in Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence (by S. Johnson)\n",
    "document = 'when a man is tired of London, he is tired of life'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a Flair sentence\n",
    "sentence = Sentence(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"when a man is tired of London, he is tired of life\"   [− Tokens: 12]\n"
     ]
    }
   ],
   "source": [
    "# print sentence\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"when a man is tired of London, he is tired of life\"   [− Tokens: 12  − Token-Labels: \"when a man is tired of London, <S-LOC> he is tired of life\"]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...OR run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize your sentence\n",
    "sentence = Sentence(document, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 when\n",
      "Token: 2 a\n",
      "Token: 3 man\n",
      "Token: 4 is\n",
      "Token: 5 tired\n",
      "Token: 6 of\n",
      "Token: 7 London\n",
      "Token: 8 ,\n",
      "Token: 9 he\n",
      "Token: 10 is\n",
      "Token: 11 tired\n",
      "Token: 12 of\n",
      "Token: 13 life\n"
     ]
    }
   ],
   "source": [
    "# inspect tokens\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "+ the segtock library is the default tokenizer of Flair\n",
    "+ custom tokenizers can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Token: 1 Red\" is tagged as \"color\" with confidence score \"1.0\"\n"
     ]
    }
   ],
   "source": [
    "# sample sentence\n",
    "sentence = Sentence('Red wine is my favourite')\n",
    "\n",
    "# get token 1 in the sentence \n",
    "token = sentence[0]\n",
    "\n",
    "# add label\n",
    "token.add_tag('ner', 'color')\n",
    "\n",
    "# get the 'ner' tag of the token\n",
    "tag = token.get_tag('ner')\n",
    "\n",
    "# print token\n",
    "print(f'\"{token}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"France is the current world cup winner.\"   [− Tokens: 7  − Sentence-Labels: {'topic': [sports (1.0), soccer (1.0)], 'language': [English (1.0)]}]\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('France is the current world cup winner.')\n",
    "\n",
    "# this sentence has multiple \"topic\" labels\n",
    "sentence.add_label('topic', 'sports')\n",
    "sentence.add_label('topic', 'soccer')\n",
    "\n",
    "# this sentence has a \"language\" labels\n",
    "sentence.add_label('language', 'English')\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports (1.0)\n",
      "soccer (1.0)\n",
      "English (1.0)\n"
     ]
    }
   ],
   "source": [
    "for label in sentence.labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging with pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('George Washington went to Washington .')\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span [1,2]: \"George Washington\"   [− Labels: PER (0.9968)]\n",
      "Span [5]: \"Washington\"   [− Labels: LOC (0.9994)]\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': [{'end_pos': 17,\n",
      "               'labels': [PER (0.9968)],\n",
      "               'start_pos': 0,\n",
      "               'text': 'George Washington'},\n",
      "              {'end_pos': 36,\n",
      "               'labels': [LOC (0.9994)],\n",
      "               'start_pos': 26,\n",
      "               'text': 'Washington'}],\n",
      " 'labels': [],\n",
      " 'text': 'George Washington went to Washington .'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(sentence.to_dict(tag_type='ner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('George <_> returned <return.01> to <_> Berlin <_> to <_> return <return.02> '\n",
      " 'his <_> hat <_> . <_>')\n",
      "'He <_> had <have.03> a <_> look <look.01> at <_> different <_> hats <_> . <_>'\n"
     ]
    }
   ],
   "source": [
    "# make English sentence\n",
    "sentence_1 = Sentence('George returned to Berlin to return his hat .')\n",
    "sentence_2 = Sentence('He had a look at different hats .')\n",
    "\n",
    "# predict NER tags\n",
    "framing.predict(sentence_1)\n",
    "framing.predict(sentence_2)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "pprint(sentence_1.to_tagged_string())\n",
    "pprint(sentence_2.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 Red\n",
      "tensor([-0.3002,  0.5015, -0.1275, -0.8164,  0.3361,  0.3221, -0.0474,  0.0371,\n",
      "        -0.6158, -0.2233, -0.3913, -0.3189,  0.8709,  0.7445,  0.2371,  0.3177,\n",
      "         0.6132, -0.4816,  0.5545, -0.4877, -0.1187,  0.1520, -0.4388,  0.0452,\n",
      "         0.6666,  0.6442, -0.2181, -0.2422,  0.1765, -0.7179,  0.4889,  0.2287,\n",
      "         0.0800,  0.1224,  0.1864,  0.2052, -0.3514,  0.8317,  0.8658,  0.3340,\n",
      "         0.4451, -0.9813, -0.1045, -0.1020,  0.6549,  0.1068, -0.0953,  0.5637,\n",
      "         0.0488, -0.1084,  0.1054,  0.0412, -0.2939,  1.0227, -0.8657, -2.5878,\n",
      "        -0.5008,  0.9758,  1.5560,  0.4521, -0.5428,  0.8199, -0.6083,  0.1992,\n",
      "         0.7497, -0.3914,  0.0605, -0.0569, -0.0121,  0.0621,  0.0706, -0.4798,\n",
      "        -0.8661, -0.5934,  0.5765,  0.9837, -0.0351,  0.4203, -0.4059,  0.3510,\n",
      "         0.8739, -0.0694, -0.6869,  0.1860, -0.3690, -0.0218, -0.1014, -0.0376,\n",
      "         0.5682,  0.7438, -0.2871, -1.0705, -0.5070, -0.1258, -0.9040, -0.2559,\n",
      "        -1.3706,  0.1731,  0.1293, -0.4852])\n",
      "Token: 2 wine\n",
      "tensor([-0.4765,  0.4209,  0.1742, -0.0558,  0.2044,  0.3640,  1.0813,  0.3260,\n",
      "         0.0963, -0.4436, -0.7446, -0.6448, -0.2799,  0.2217, -1.0897,  0.2020,\n",
      "         0.8254, -0.5224, -0.7204,  0.8119, -0.6910,  0.1812, -0.3226, -0.8353,\n",
      "        -0.1770,  0.5722, -0.3487, -0.7293, -0.4237, -1.0600, -0.1819, -0.0844,\n",
      "         0.0627, -1.3184,  0.5756,  1.1970,  0.3082,  0.1086,  0.2251, -0.6283,\n",
      "         0.0917, -0.8229, -0.0500, -0.8649,  1.0282, -0.1852, -0.6962, -0.8606,\n",
      "         0.2283, -0.9593,  0.7146, -0.0528, -0.6747,  0.0447, -1.1310, -1.5243,\n",
      "        -1.0981,  0.5774,  1.2551, -0.6494,  0.5922,  0.6989,  0.1473, -0.0145,\n",
      "        -0.0366, -0.5596,  0.7257, -0.8536,  0.8610, -0.7257,  0.2120,  0.9780,\n",
      "         0.7839,  0.2417, -0.3819, -0.2503, -0.1283, -0.8077, -0.4196,  0.2372,\n",
      "         0.2957,  0.0673, -0.7731,  0.6046,  0.0396, -0.4549, -0.6280, -0.5806,\n",
      "        -0.2288, -0.2866, -0.5002,  0.0092, -0.5545, -0.9151, -0.9261,  0.4541,\n",
      "        -0.6398, -0.5445,  0.8034,  0.5008])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
      "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
      "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
      "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
      "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
      "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
      "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
      "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
      "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
      "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
      "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
      "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
      "        -0.0442, -1.2969,  0.7622,  0.4635])\n",
      "Token: 4 my\n",
      "tensor([ 0.0803, -0.1086,  0.7207, -0.4514, -0.7496,  0.6378, -0.2571,  0.4161,\n",
      "        -0.0545,  0.3556,  0.3586,  0.5400,  0.4912,  0.2571, -0.2147, -0.4284,\n",
      "        -0.4232,  0.3872, -0.3569,  0.4012, -0.1985,  0.4345, -0.3648,  0.0717,\n",
      "         0.5332,  0.8456, -0.6754, -1.2527,  0.8376, -0.1593,  0.3780,  0.9454,\n",
      "         0.8307,  0.1943, -0.5845,  0.5828, -0.6256,  0.4904,  0.4327, -0.5425,\n",
      "         0.1045, -0.1626,  0.9900, -0.7422, -0.5978,  0.1019, -0.3357, -0.3909,\n",
      "         0.1513, -1.3533, -0.1126,  0.1435,  0.0381,  1.1167, -0.2308, -2.6394,\n",
      "         0.6685,  0.4845,  1.8796,  0.0803,  0.7373,  1.8058, -0.5193,  0.0041,\n",
      "         0.7699,  0.3688,  0.8114,  0.1694, -0.1192, -0.2650,  0.2269,  0.7694,\n",
      "         0.8520, -0.9777,  0.2318,  0.8814, -0.2709, -0.3991, -0.5719,  0.0756,\n",
      "         0.1809,  0.5904, -0.1343, -0.1206, -1.8157, -0.3550, -0.3172, -0.2704,\n",
      "        -0.6723, -0.0410, -0.4433,  0.3565,  1.0247,  0.4969, -0.5170, -0.4928,\n",
      "        -0.3340, -0.3484,  0.3147,  1.0087])\n",
      "Token: 5 favourite\n",
      "tensor([-0.1940,  0.3541,  0.2822,  0.1724,  0.2704,  0.1396,  0.1591, -0.7720,\n",
      "        -0.3762,  0.3975, -0.1416,  0.4423, -0.3516, -0.5530, -0.4717, -0.6723,\n",
      "         0.5679, -0.4082,  0.5990,  0.4092,  0.2158,  0.0971,  0.2438,  0.0473,\n",
      "         0.3074, -0.3747,  0.5012, -0.0125,  0.5104,  0.0588, -0.6519, -0.6154,\n",
      "         0.8037,  0.3766,  0.4369,  0.5948, -1.3332,  0.3028, -0.4533, -0.4701,\n",
      "         0.3375,  0.4101,  0.7957, -0.5891,  0.8702, -0.5519, -0.6112, -0.3940,\n",
      "         0.0656, -0.7240, -0.0208, -0.3677,  0.6728,  0.8627,  0.4074, -1.7477,\n",
      "        -0.4480,  1.0907, -0.1935, -0.5389,  0.1330,  0.1120,  0.5552,  0.8870,\n",
      "         0.9826, -0.0551, -0.1993,  0.0732,  0.0521, -0.5622, -0.7097,  0.0709,\n",
      "         0.1440,  0.0170, -0.8081,  0.3981, -0.0875, -0.2512,  0.5771, -0.1972,\n",
      "         0.1540, -0.1549, -0.0063, -0.0246, -0.4299, -1.1216, -0.9036,  0.5811,\n",
      "         0.0447,  0.0916, -0.8148,  0.4499,  0.5824,  0.4512, -0.7870, -0.1836,\n",
      "        -0.7215, -0.2009,  0.2092,  0.4484])\n"
     ]
    }
   ],
   "source": [
    "# sample sentence\n",
    "sentence = Sentence('Red wine is my favourite')\n",
    "\n",
    "# embed a sentence using glove.\n",
    "glove_embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual string embeddings are powerful embeddings that capture latent syntactic-semantic information that goes beyond standard word embeddings.\n",
    "\n",
    "Key differences are: \n",
    "\n",
    "1. they are trained without any explicit notion of words and thus fundamentally model words as sequences of characters\n",
    "2. they are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"Red wine is my favourite\"   [− Tokens: 5]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('Red wine is my favourite')\n",
    "\n",
    "# embed words in sentence\n",
    "flair_embedding_forward.embed(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 Red\n",
      "tensor([-0.0002,  0.0018, -0.0321,  ...,  0.0012,  0.0179,  0.0066])\n",
      "Token: 2 wine\n",
      "tensor([-0.0008,  0.0027,  0.0160,  ..., -0.0028, -0.0090,  0.0397])\n",
      "Token: 3 is\n",
      "tensor([ 0.0008, -0.0015,  0.0603,  ..., -0.0050,  0.0121,  0.0107])\n",
      "Token: 4 my\n",
      "tensor([ 8.9246e-05,  8.9543e-05,  2.1088e-02,  ..., -6.4441e-04,\n",
      "         4.2823e-02,  1.8494e-03])\n",
      "Token: 5 favourite\n",
      "tensor([-9.1581e-04, -4.5748e-05,  1.9024e-02,  ..., -3.2326e-04,\n",
      "        -1.3051e-04,  2.0492e-02])\n"
     ]
    }
   ],
   "source": [
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following word embeddings are currently supported: \n",
    "\n",
    "| Class | Type | Paper | \n",
    "| ------------- | -------------  | -------------  | \n",
    "| [`BytePairEmbeddings`](/resources/docs/embeddings/BYTE_PAIR_EMBEDDINGS.md) | Subword-level word embeddings | [Heinzerling and Strube (2018)](https://www.aclweb.org/anthology/L18-1473)  |\n",
    "| [`CharacterEmbeddings`](/resources/docs/embeddings/CHARACTER_EMBEDDINGS.md) | Task-trained character-level embeddings of words | [Lample et al. (2016)](https://www.aclweb.org/anthology/N16-1030) |\n",
    "| [`ELMoEmbeddings`](/resources/docs/embeddings/ELMO_EMBEDDINGS.md) | Contextualized word-level embeddings | [Peters et al. (2018)](https://aclweb.org/anthology/N18-1202)  |\n",
    "| [`FastTextEmbeddings`](/resources/docs/embeddings/FASTTEXT_EMBEDDINGS.md) | Word embeddings with subword features | [Bojanowski et al. (2017)](https://aclweb.org/anthology/Q17-1010)  |\n",
    "| [`FlairEmbeddings`](/resources/docs/embeddings/FLAIR_EMBEDDINGS.md) | Contextualized character-level embeddings | [Akbik et al. (2018)](https://www.aclweb.org/anthology/C18-1139/)  |\n",
    "| [`OneHotEmbeddings`](/resources/docs/embeddings/ONE_HOT_EMBEDDINGS.md) | Standard one-hot embeddings of text or tags | - |\n",
    "| [`PooledFlairEmbeddings`](/resources/docs/embeddings/FLAIR_EMBEDDINGS.md) | Pooled variant of `FlairEmbeddings` |  [Akbik et al. (2019)](https://www.aclweb.org/anthology/N19-1078/)  | \n",
    "| [`TransformerWordEmbeddings`](/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md) | Embeddings from pretrained [transformers](https://huggingface.co/transformers/pretrained_models.html) (BERT, XLM, GPT, RoBERTa, XLNet, DistilBERT etc.) | [Devlin et al. (2018)](https://www.aclweb.org/anthology/N19-1423/) [Radford et al. (2018)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  [Liu et al. (2019)](https://arxiv.org/abs/1907.11692) [Dai et al. (2019)](https://arxiv.org/abs/1901.02860) [Yang et al. (2019)](https://arxiv.org/abs/1906.08237) [Lample and Conneau (2019)](https://arxiv.org/abs/1901.07291) |  \n",
    "| [`WordEmbeddings`](/resources/docs/embeddings/CLASSIC_WORD_EMBEDDINGS.md) | Classic word embeddings |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario A: using annotated (prepared) datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/_24.png' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario B: using your own annotated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "George N B-PER\n",
    "Washington N I-PER\n",
    "went V O\n",
    "to P O\n",
    "Washington N B-LOC\n",
    "\n",
    "Sam N B-PER\n",
    "Houston N I-PER\n",
    "stayed V O\n",
    "home N O\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '/path/to/data/folder'\n",
    "\n",
    "# # init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "# corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "#                               train_file='train.txt',\n",
    "#                               test_file='test.txt',\n",
    "#                               dev_file='dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get back to Scenario A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-24 11:34:59,511 Reading data from /home/simone/.flair/datasets/ud_english\n",
      "2020-06-24 11:34:59,512 Train: /home/simone/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-06-24 11:34:59,512 Dev: /home/simone/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2020-06-24 11:34:59,513 Test: /home/simone/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "Corpus: 1254 train + 200 dev + 208 test sentences\n",
      "Dictionary with 53 tags: <unk>, O, PRP, RB, VBP, IN, VB, VBN, JJR, NNP, ., DT, JJ, NN, ,, VBD, NNS, CC, VBG, MD, EX, CD, PRP$, WP, POS, VBZ, TO, WRB, JJS, UH\n",
      "2020-06-24 11:35:11,812 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:11,813 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=53, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-06-24 11:35:11,813 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:11,814 Corpus: \"Corpus: 1254 train + 200 dev + 208 test sentences\"\n",
      "2020-06-24 11:35:11,814 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:11,815 Parameters:\n",
      "2020-06-24 11:35:11,815  - learning_rate: \"0.1\"\n",
      "2020-06-24 11:35:11,817  - mini_batch_size: \"32\"\n",
      "2020-06-24 11:35:11,817  - patience: \"3\"\n",
      "2020-06-24 11:35:11,818  - anneal_factor: \"0.5\"\n",
      "2020-06-24 11:35:11,818  - max_epochs: \"150\"\n",
      "2020-06-24 11:35:11,819  - shuffle: \"True\"\n",
      "2020-06-24 11:35:11,819  - train_with_dev: \"False\"\n",
      "2020-06-24 11:35:11,821  - batch_growth_annealing: \"False\"\n",
      "2020-06-24 11:35:11,821 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:11,822 Model training base path: \"resources/taggers/example-pos\"\n",
      "2020-06-24 11:35:11,822 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:11,823 Device: cpu\n",
      "2020-06-24 11:35:11,824 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:11,824 Embeddings storage mode: cpu\n",
      "2020-06-24 11:35:11,826 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:13,187 epoch 1 - iter 4/40 - loss 73.49939728 - samples/sec: 94.15\n",
      "2020-06-24 11:35:14,789 epoch 1 - iter 8/40 - loss 68.21634436 - samples/sec: 80.49\n",
      "2020-06-24 11:35:16,057 epoch 1 - iter 12/40 - loss 63.69611295 - samples/sec: 101.69\n",
      "2020-06-24 11:35:17,217 epoch 1 - iter 16/40 - loss 60.52280211 - samples/sec: 111.36\n",
      "2020-06-24 11:35:18,540 epoch 1 - iter 20/40 - loss 58.51271324 - samples/sec: 97.60\n",
      "2020-06-24 11:35:19,954 epoch 1 - iter 24/40 - loss 56.95888710 - samples/sec: 91.08\n",
      "2020-06-24 11:35:21,110 epoch 1 - iter 28/40 - loss 55.55687727 - samples/sec: 111.67\n",
      "2020-06-24 11:35:22,184 epoch 1 - iter 32/40 - loss 54.21030426 - samples/sec: 120.36\n",
      "2020-06-24 11:35:23,302 epoch 1 - iter 36/40 - loss 53.28233687 - samples/sec: 115.53\n",
      "2020-06-24 11:35:24,512 epoch 1 - iter 40/40 - loss 53.15836563 - samples/sec: 106.75\n",
      "2020-06-24 11:35:24,520 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:24,521 EPOCH 1 done: loss 53.1584 - lr 0.1000000\n",
      "2020-06-24 11:35:25,016 DEV : loss 29.557079315185547 - score 0.2664\n",
      "2020-06-24 11:35:25,024 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:35:28,067 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:29,319 epoch 2 - iter 4/40 - loss 49.61518383 - samples/sec: 102.43\n",
      "2020-06-24 11:35:30,747 epoch 2 - iter 8/40 - loss 46.53905916 - samples/sec: 90.26\n",
      "2020-06-24 11:35:32,011 epoch 2 - iter 12/40 - loss 44.75366624 - samples/sec: 101.90\n",
      "2020-06-24 11:35:33,355 epoch 2 - iter 16/40 - loss 45.20527387 - samples/sec: 96.03\n",
      "2020-06-24 11:35:34,504 epoch 2 - iter 20/40 - loss 44.71640854 - samples/sec: 112.39\n",
      "2020-06-24 11:35:35,700 epoch 2 - iter 24/40 - loss 43.91561445 - samples/sec: 107.95\n",
      "2020-06-24 11:35:36,900 epoch 2 - iter 28/40 - loss 42.84909773 - samples/sec: 107.81\n",
      "2020-06-24 11:35:38,290 epoch 2 - iter 32/40 - loss 42.50406247 - samples/sec: 92.67\n",
      "2020-06-24 11:35:39,481 epoch 2 - iter 36/40 - loss 41.84189971 - samples/sec: 108.32\n",
      "2020-06-24 11:35:40,226 epoch 2 - iter 40/40 - loss 40.78123765 - samples/sec: 173.92\n",
      "2020-06-24 11:35:40,236 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:40,236 EPOCH 2 done: loss 40.7812 - lr 0.1000000\n",
      "2020-06-24 11:35:40,710 DEV : loss 22.730993270874023 - score 0.4462\n",
      "2020-06-24 11:35:40,718 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:35:44,257 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:45,431 epoch 3 - iter 4/40 - loss 35.67096043 - samples/sec: 109.26\n",
      "2020-06-24 11:35:46,665 epoch 3 - iter 8/40 - loss 36.26897240 - samples/sec: 104.40\n",
      "2020-06-24 11:35:47,965 epoch 3 - iter 12/40 - loss 35.83316882 - samples/sec: 99.29\n",
      "2020-06-24 11:35:49,289 epoch 3 - iter 16/40 - loss 35.40420723 - samples/sec: 97.46\n",
      "2020-06-24 11:35:50,473 epoch 3 - iter 20/40 - loss 34.93672953 - samples/sec: 108.95\n",
      "2020-06-24 11:35:51,704 epoch 3 - iter 24/40 - loss 34.95357172 - samples/sec: 104.87\n",
      "2020-06-24 11:35:52,869 epoch 3 - iter 28/40 - loss 34.29655572 - samples/sec: 110.72\n",
      "2020-06-24 11:35:54,289 epoch 3 - iter 32/40 - loss 34.17916185 - samples/sec: 90.78\n",
      "2020-06-24 11:35:55,429 epoch 3 - iter 36/40 - loss 33.84336609 - samples/sec: 113.40\n",
      "2020-06-24 11:35:56,909 epoch 3 - iter 40/40 - loss 33.89535856 - samples/sec: 87.05\n",
      "2020-06-24 11:35:56,919 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:35:56,920 EPOCH 3 done: loss 33.8954 - lr 0.1000000\n",
      "2020-06-24 11:35:57,458 DEV : loss 18.454727172851562 - score 0.5398\n",
      "2020-06-24 11:35:57,466 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:36:00,949 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:02,365 epoch 4 - iter 4/40 - loss 30.66524792 - samples/sec: 90.60\n",
      "2020-06-24 11:36:03,570 epoch 4 - iter 8/40 - loss 30.11472487 - samples/sec: 107.03\n",
      "2020-06-24 11:36:04,602 epoch 4 - iter 12/40 - loss 30.09524695 - samples/sec: 125.22\n",
      "2020-06-24 11:36:05,593 epoch 4 - iter 16/40 - loss 29.61694479 - samples/sec: 130.44\n",
      "2020-06-24 11:36:06,563 epoch 4 - iter 20/40 - loss 29.16666059 - samples/sec: 133.39\n",
      "2020-06-24 11:36:08,058 epoch 4 - iter 24/40 - loss 29.49397683 - samples/sec: 86.23\n",
      "2020-06-24 11:36:08,995 epoch 4 - iter 28/40 - loss 28.63681528 - samples/sec: 138.05\n",
      "2020-06-24 11:36:10,604 epoch 4 - iter 32/40 - loss 28.73534906 - samples/sec: 80.10\n",
      "2020-06-24 11:36:11,870 epoch 4 - iter 36/40 - loss 28.84501595 - samples/sec: 102.03\n",
      "2020-06-24 11:36:12,882 epoch 4 - iter 40/40 - loss 28.77463169 - samples/sec: 127.58\n",
      "2020-06-24 11:36:12,890 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:12,891 EPOCH 4 done: loss 28.7746 - lr 0.1000000\n",
      "2020-06-24 11:36:13,364 DEV : loss 14.629263877868652 - score 0.6364\n",
      "2020-06-24 11:36:13,371 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:36:16,963 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:18,252 epoch 5 - iter 4/40 - loss 25.82622719 - samples/sec: 99.52\n",
      "2020-06-24 11:36:19,724 epoch 5 - iter 8/40 - loss 26.51326323 - samples/sec: 87.56\n",
      "2020-06-24 11:36:21,092 epoch 5 - iter 12/40 - loss 27.29174805 - samples/sec: 94.25\n",
      "2020-06-24 11:36:22,254 epoch 5 - iter 16/40 - loss 27.13657463 - samples/sec: 111.18\n",
      "2020-06-24 11:36:23,304 epoch 5 - iter 20/40 - loss 26.42246103 - samples/sec: 123.01\n",
      "2020-06-24 11:36:24,569 epoch 5 - iter 24/40 - loss 26.31376815 - samples/sec: 102.04\n",
      "2020-06-24 11:36:25,887 epoch 5 - iter 28/40 - loss 26.11788804 - samples/sec: 97.78\n",
      "2020-06-24 11:36:26,839 epoch 5 - iter 32/40 - loss 25.87828666 - samples/sec: 135.90\n",
      "2020-06-24 11:36:27,805 epoch 5 - iter 36/40 - loss 25.70109346 - samples/sec: 134.12\n",
      "2020-06-24 11:36:28,665 epoch 5 - iter 40/40 - loss 25.57509851 - samples/sec: 150.54\n",
      "2020-06-24 11:36:28,673 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:28,674 EPOCH 5 done: loss 25.5751 - lr 0.1000000\n",
      "2020-06-24 11:36:29,155 DEV : loss 13.127612113952637 - score 0.6602\n",
      "2020-06-24 11:36:29,162 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:36:32,747 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:33,765 epoch 6 - iter 4/40 - loss 23.31210423 - samples/sec: 126.09\n",
      "2020-06-24 11:36:35,312 epoch 6 - iter 8/40 - loss 26.33358598 - samples/sec: 83.29\n",
      "2020-06-24 11:36:36,582 epoch 6 - iter 12/40 - loss 26.26510747 - samples/sec: 101.62\n",
      "2020-06-24 11:36:37,727 epoch 6 - iter 16/40 - loss 25.85073090 - samples/sec: 112.84\n",
      "2020-06-24 11:36:38,588 epoch 6 - iter 20/40 - loss 25.01489172 - samples/sec: 150.32\n",
      "2020-06-24 11:36:39,645 epoch 6 - iter 24/40 - loss 24.28287665 - samples/sec: 122.23\n",
      "2020-06-24 11:36:40,732 epoch 6 - iter 28/40 - loss 23.85125242 - samples/sec: 118.79\n",
      "2020-06-24 11:36:41,970 epoch 6 - iter 32/40 - loss 23.82897615 - samples/sec: 104.20\n",
      "2020-06-24 11:36:43,471 epoch 6 - iter 36/40 - loss 23.78208627 - samples/sec: 85.87\n",
      "2020-06-24 11:36:44,706 epoch 6 - iter 40/40 - loss 23.40834789 - samples/sec: 104.39\n",
      "2020-06-24 11:36:44,718 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:44,719 EPOCH 6 done: loss 23.4083 - lr 0.1000000\n",
      "2020-06-24 11:36:45,294 DEV : loss 11.939422607421875 - score 0.6957\n",
      "2020-06-24 11:36:45,302 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:36:48,853 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:36:49,933 epoch 7 - iter 4/40 - loss 21.68476200 - samples/sec: 118.90\n",
      "2020-06-24 11:36:51,256 epoch 7 - iter 8/40 - loss 22.01262975 - samples/sec: 97.45\n",
      "2020-06-24 11:36:52,214 epoch 7 - iter 12/40 - loss 21.58529154 - samples/sec: 134.89\n",
      "2020-06-24 11:36:53,413 epoch 7 - iter 16/40 - loss 21.45654011 - samples/sec: 107.72\n",
      "2020-06-24 11:36:55,010 epoch 7 - iter 20/40 - loss 21.54375668 - samples/sec: 80.62\n",
      "2020-06-24 11:36:56,130 epoch 7 - iter 24/40 - loss 21.39238373 - samples/sec: 115.15\n",
      "2020-06-24 11:36:57,338 epoch 7 - iter 28/40 - loss 21.75259815 - samples/sec: 106.83\n",
      "2020-06-24 11:36:58,673 epoch 7 - iter 32/40 - loss 21.95788980 - samples/sec: 96.58\n",
      "2020-06-24 11:36:59,906 epoch 7 - iter 36/40 - loss 21.98977576 - samples/sec: 104.67\n",
      "2020-06-24 11:37:01,013 epoch 7 - iter 40/40 - loss 21.72406368 - samples/sec: 116.66\n",
      "2020-06-24 11:37:01,022 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:01,023 EPOCH 7 done: loss 21.7241 - lr 0.1000000\n",
      "2020-06-24 11:37:01,529 DEV : loss 11.031184196472168 - score 0.7069\n",
      "2020-06-24 11:37:01,538 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:37:05,111 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:06,328 epoch 8 - iter 4/40 - loss 20.82636213 - samples/sec: 105.40\n",
      "2020-06-24 11:37:07,557 epoch 8 - iter 8/40 - loss 21.09407592 - samples/sec: 105.06\n",
      "2020-06-24 11:37:08,682 epoch 8 - iter 12/40 - loss 21.14077711 - samples/sec: 115.07\n",
      "2020-06-24 11:37:09,975 epoch 8 - iter 16/40 - loss 21.15099573 - samples/sec: 99.77\n",
      "2020-06-24 11:37:11,232 epoch 8 - iter 20/40 - loss 20.99045887 - samples/sec: 102.66\n",
      "2020-06-24 11:37:12,439 epoch 8 - iter 24/40 - loss 20.87831457 - samples/sec: 107.04\n",
      "2020-06-24 11:37:14,007 epoch 8 - iter 28/40 - loss 21.02953666 - samples/sec: 82.13\n",
      "2020-06-24 11:37:15,336 epoch 8 - iter 32/40 - loss 20.87130883 - samples/sec: 97.02\n",
      "2020-06-24 11:37:17,072 epoch 8 - iter 36/40 - loss 20.95545427 - samples/sec: 74.11\n",
      "2020-06-24 11:37:18,007 epoch 8 - iter 40/40 - loss 20.73857028 - samples/sec: 138.18\n",
      "2020-06-24 11:37:18,015 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:18,016 EPOCH 8 done: loss 20.7386 - lr 0.1000000\n",
      "2020-06-24 11:37:18,525 DEV : loss 10.523231506347656 - score 0.7169\n",
      "2020-06-24 11:37:18,533 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:37:22,408 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:24,009 epoch 9 - iter 4/40 - loss 22.51796341 - samples/sec: 80.05\n",
      "2020-06-24 11:37:25,941 epoch 9 - iter 8/40 - loss 21.06852508 - samples/sec: 66.58\n",
      "2020-06-24 11:37:27,302 epoch 9 - iter 12/40 - loss 21.15633996 - samples/sec: 94.82\n",
      "2020-06-24 11:37:28,436 epoch 9 - iter 16/40 - loss 20.71926689 - samples/sec: 113.72\n",
      "2020-06-24 11:37:29,411 epoch 9 - iter 20/40 - loss 19.93723788 - samples/sec: 132.55\n",
      "2020-06-24 11:37:30,702 epoch 9 - iter 24/40 - loss 20.08721300 - samples/sec: 99.90\n",
      "2020-06-24 11:37:32,052 epoch 9 - iter 28/40 - loss 19.68235680 - samples/sec: 95.63\n",
      "2020-06-24 11:37:33,722 epoch 9 - iter 32/40 - loss 19.72279456 - samples/sec: 77.05\n",
      "2020-06-24 11:37:35,018 epoch 9 - iter 36/40 - loss 19.82809583 - samples/sec: 99.60\n",
      "2020-06-24 11:37:36,077 epoch 9 - iter 40/40 - loss 19.54983323 - samples/sec: 122.01\n",
      "2020-06-24 11:37:36,087 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:36,088 EPOCH 9 done: loss 19.5498 - lr 0.1000000\n",
      "2020-06-24 11:37:36,637 DEV : loss 9.60346794128418 - score 0.7466\n",
      "2020-06-24 11:37:36,644 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:37:40,284 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:41,438 epoch 10 - iter 4/40 - loss 17.59594154 - samples/sec: 111.22\n",
      "2020-06-24 11:37:42,640 epoch 10 - iter 8/40 - loss 18.32297730 - samples/sec: 107.49\n",
      "2020-06-24 11:37:43,998 epoch 10 - iter 12/40 - loss 18.91168928 - samples/sec: 94.89\n",
      "2020-06-24 11:37:45,648 epoch 10 - iter 16/40 - loss 19.30265009 - samples/sec: 78.00\n",
      "2020-06-24 11:37:47,000 epoch 10 - iter 20/40 - loss 19.15197668 - samples/sec: 95.44\n",
      "2020-06-24 11:37:48,082 epoch 10 - iter 24/40 - loss 19.00143421 - samples/sec: 119.25\n",
      "2020-06-24 11:37:49,148 epoch 10 - iter 28/40 - loss 18.85734585 - samples/sec: 121.21\n",
      "2020-06-24 11:37:50,357 epoch 10 - iter 32/40 - loss 18.75348422 - samples/sec: 106.79\n",
      "2020-06-24 11:37:51,704 epoch 10 - iter 36/40 - loss 18.82347274 - samples/sec: 95.64\n",
      "2020-06-24 11:37:52,674 epoch 10 - iter 40/40 - loss 19.03034799 - samples/sec: 133.32\n",
      "2020-06-24 11:37:52,685 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:52,686 EPOCH 10 done: loss 19.0303 - lr 0.1000000\n",
      "2020-06-24 11:37:53,176 DEV : loss 9.251897811889648 - score 0.7538\n",
      "2020-06-24 11:37:53,183 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:37:56,806 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:37:58,127 epoch 11 - iter 4/40 - loss 17.19087958 - samples/sec: 97.11\n",
      "2020-06-24 11:37:59,089 epoch 11 - iter 8/40 - loss 17.20426273 - samples/sec: 134.48\n",
      "2020-06-24 11:38:00,212 epoch 11 - iter 12/40 - loss 17.06385644 - samples/sec: 115.24\n",
      "2020-06-24 11:38:01,352 epoch 11 - iter 16/40 - loss 16.98335344 - samples/sec: 113.40\n",
      "2020-06-24 11:38:03,693 epoch 11 - iter 20/40 - loss 17.35482826 - samples/sec: 54.92\n",
      "2020-06-24 11:38:04,917 epoch 11 - iter 24/40 - loss 17.27411449 - samples/sec: 105.38\n",
      "2020-06-24 11:38:06,337 epoch 11 - iter 28/40 - loss 17.50187285 - samples/sec: 90.89\n",
      "2020-06-24 11:38:07,661 epoch 11 - iter 32/40 - loss 17.85377553 - samples/sec: 97.37\n",
      "2020-06-24 11:38:08,962 epoch 11 - iter 36/40 - loss 18.08554048 - samples/sec: 99.20\n",
      "2020-06-24 11:38:10,252 epoch 11 - iter 40/40 - loss 18.23484809 - samples/sec: 99.97\n",
      "2020-06-24 11:38:10,262 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:10,263 EPOCH 11 done: loss 18.2348 - lr 0.1000000\n",
      "2020-06-24 11:38:10,773 DEV : loss 8.846505165100098 - score 0.7658\n",
      "2020-06-24 11:38:10,782 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:38:14,446 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:15,738 epoch 12 - iter 4/40 - loss 18.21879816 - samples/sec: 99.26\n",
      "2020-06-24 11:38:17,128 epoch 12 - iter 8/40 - loss 17.82547164 - samples/sec: 92.77\n",
      "2020-06-24 11:38:18,361 epoch 12 - iter 12/40 - loss 18.53925705 - samples/sec: 104.79\n",
      "2020-06-24 11:38:19,381 epoch 12 - iter 16/40 - loss 18.03819615 - samples/sec: 126.64\n",
      "2020-06-24 11:38:20,824 epoch 12 - iter 20/40 - loss 17.93118739 - samples/sec: 89.43\n",
      "2020-06-24 11:38:21,864 epoch 12 - iter 24/40 - loss 17.69224469 - samples/sec: 124.12\n",
      "2020-06-24 11:38:23,129 epoch 12 - iter 28/40 - loss 17.45324714 - samples/sec: 101.89\n",
      "2020-06-24 11:38:24,360 epoch 12 - iter 32/40 - loss 17.38235486 - samples/sec: 104.72\n",
      "2020-06-24 11:38:25,577 epoch 12 - iter 36/40 - loss 17.64819145 - samples/sec: 106.14\n",
      "2020-06-24 11:38:26,580 epoch 12 - iter 40/40 - loss 17.77282522 - samples/sec: 128.90\n",
      "2020-06-24 11:38:26,588 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:26,589 EPOCH 12 done: loss 17.7728 - lr 0.1000000\n",
      "2020-06-24 11:38:27,070 DEV : loss 8.3197021484375 - score 0.7785\n",
      "2020-06-24 11:38:27,077 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:38:30,697 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:32,029 epoch 13 - iter 4/40 - loss 17.12260509 - samples/sec: 96.33\n",
      "2020-06-24 11:38:33,074 epoch 13 - iter 8/40 - loss 16.06373107 - samples/sec: 123.67\n",
      "2020-06-24 11:38:34,310 epoch 13 - iter 12/40 - loss 17.46740572 - samples/sec: 104.41\n",
      "2020-06-24 11:38:35,471 epoch 13 - iter 16/40 - loss 16.93301201 - samples/sec: 111.31\n",
      "2020-06-24 11:38:36,472 epoch 13 - iter 20/40 - loss 16.89265924 - samples/sec: 129.32\n",
      "2020-06-24 11:38:37,896 epoch 13 - iter 24/40 - loss 17.07780723 - samples/sec: 90.45\n",
      "2020-06-24 11:38:39,104 epoch 13 - iter 28/40 - loss 16.86595413 - samples/sec: 106.79\n",
      "2020-06-24 11:38:40,300 epoch 13 - iter 32/40 - loss 16.65280735 - samples/sec: 107.96\n",
      "2020-06-24 11:38:41,482 epoch 13 - iter 36/40 - loss 16.77605894 - samples/sec: 109.31\n",
      "2020-06-24 11:38:42,658 epoch 13 - iter 40/40 - loss 16.75617399 - samples/sec: 109.84\n",
      "2020-06-24 11:38:42,667 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:42,668 EPOCH 13 done: loss 16.7562 - lr 0.1000000\n",
      "2020-06-24 11:38:43,147 DEV : loss 8.148077964782715 - score 0.7804\n",
      "2020-06-24 11:38:43,155 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:38:46,881 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:48,276 epoch 14 - iter 4/40 - loss 16.58468223 - samples/sec: 91.83\n",
      "2020-06-24 11:38:49,295 epoch 14 - iter 8/40 - loss 16.34492409 - samples/sec: 126.96\n",
      "2020-06-24 11:38:50,575 epoch 14 - iter 12/40 - loss 16.85229794 - samples/sec: 100.91\n",
      "2020-06-24 11:38:51,847 epoch 14 - iter 16/40 - loss 16.68834156 - samples/sec: 101.36\n",
      "2020-06-24 11:38:52,858 epoch 14 - iter 20/40 - loss 16.13523302 - samples/sec: 127.74\n",
      "2020-06-24 11:38:54,166 epoch 14 - iter 24/40 - loss 16.30108507 - samples/sec: 98.54\n",
      "2020-06-24 11:38:55,583 epoch 14 - iter 28/40 - loss 16.48525977 - samples/sec: 90.88\n",
      "2020-06-24 11:38:56,734 epoch 14 - iter 32/40 - loss 16.47844014 - samples/sec: 112.29\n",
      "2020-06-24 11:38:57,901 epoch 14 - iter 36/40 - loss 16.53926963 - samples/sec: 110.58\n",
      "2020-06-24 11:38:58,984 epoch 14 - iter 40/40 - loss 16.35137630 - samples/sec: 119.30\n",
      "2020-06-24 11:38:58,994 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:38:58,994 EPOCH 14 done: loss 16.3514 - lr 0.1000000\n",
      "2020-06-24 11:38:59,450 DEV : loss 8.169198036193848 - score 0.7717\n",
      "2020-06-24 11:38:59,457 BAD EPOCHS (no improvement): 1\n",
      "2020-06-24 11:38:59,458 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:01,092 epoch 15 - iter 4/40 - loss 17.98219776 - samples/sec: 78.37\n",
      "2020-06-24 11:39:02,232 epoch 15 - iter 8/40 - loss 16.81906652 - samples/sec: 113.35\n",
      "2020-06-24 11:39:03,787 epoch 15 - iter 12/40 - loss 17.45362123 - samples/sec: 82.74\n",
      "2020-06-24 11:39:04,839 epoch 15 - iter 16/40 - loss 16.87884420 - samples/sec: 122.80\n",
      "2020-06-24 11:39:05,940 epoch 15 - iter 20/40 - loss 16.42141805 - samples/sec: 117.22\n",
      "2020-06-24 11:39:07,393 epoch 15 - iter 24/40 - loss 16.28794360 - samples/sec: 88.62\n",
      "2020-06-24 11:39:08,607 epoch 15 - iter 28/40 - loss 16.17865116 - samples/sec: 106.29\n",
      "2020-06-24 11:39:10,185 epoch 15 - iter 32/40 - loss 16.11239770 - samples/sec: 81.55\n",
      "2020-06-24 11:39:11,450 epoch 15 - iter 36/40 - loss 16.09543663 - samples/sec: 102.11\n",
      "2020-06-24 11:39:12,468 epoch 15 - iter 40/40 - loss 16.09727426 - samples/sec: 127.20\n",
      "2020-06-24 11:39:12,478 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:12,479 EPOCH 15 done: loss 16.0973 - lr 0.1000000\n",
      "2020-06-24 11:39:12,968 DEV : loss 7.650144100189209 - score 0.7921\n",
      "2020-06-24 11:39:12,976 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:39:16,853 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:18,291 epoch 16 - iter 4/40 - loss 17.03317070 - samples/sec: 89.19\n",
      "2020-06-24 11:39:19,429 epoch 16 - iter 8/40 - loss 15.74883568 - samples/sec: 113.45\n",
      "2020-06-24 11:39:20,963 epoch 16 - iter 12/40 - loss 16.28215973 - samples/sec: 83.98\n",
      "2020-06-24 11:39:22,252 epoch 16 - iter 16/40 - loss 15.73688430 - samples/sec: 100.18\n",
      "2020-06-24 11:39:23,676 epoch 16 - iter 20/40 - loss 15.97210965 - samples/sec: 90.54\n",
      "2020-06-24 11:39:24,925 epoch 16 - iter 24/40 - loss 15.94430904 - samples/sec: 103.40\n",
      "2020-06-24 11:39:25,983 epoch 16 - iter 28/40 - loss 15.98851323 - samples/sec: 122.10\n",
      "2020-06-24 11:39:27,400 epoch 16 - iter 32/40 - loss 16.01230484 - samples/sec: 90.95\n",
      "2020-06-24 11:39:28,312 epoch 16 - iter 36/40 - loss 15.69009084 - samples/sec: 141.68\n",
      "2020-06-24 11:39:29,415 epoch 16 - iter 40/40 - loss 15.75730517 - samples/sec: 117.15\n",
      "2020-06-24 11:39:29,424 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:29,425 EPOCH 16 done: loss 15.7573 - lr 0.1000000\n",
      "2020-06-24 11:39:30,060 DEV : loss 7.284870147705078 - score 0.7978\n",
      "2020-06-24 11:39:30,070 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:39:33,809 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:35,082 epoch 17 - iter 4/40 - loss 17.03145623 - samples/sec: 100.75\n",
      "2020-06-24 11:39:36,300 epoch 17 - iter 8/40 - loss 15.85124743 - samples/sec: 106.32\n",
      "2020-06-24 11:39:37,726 epoch 17 - iter 12/40 - loss 15.89316297 - samples/sec: 90.32\n",
      "2020-06-24 11:39:38,910 epoch 17 - iter 16/40 - loss 15.63218659 - samples/sec: 109.21\n",
      "2020-06-24 11:39:40,570 epoch 17 - iter 20/40 - loss 15.65198336 - samples/sec: 77.64\n",
      "2020-06-24 11:39:41,861 epoch 17 - iter 24/40 - loss 15.36838182 - samples/sec: 100.00\n",
      "2020-06-24 11:39:43,066 epoch 17 - iter 28/40 - loss 15.40089464 - samples/sec: 107.17\n",
      "2020-06-24 11:39:44,103 epoch 17 - iter 32/40 - loss 15.36580044 - samples/sec: 124.46\n",
      "2020-06-24 11:39:45,190 epoch 17 - iter 36/40 - loss 15.20664390 - samples/sec: 118.84\n",
      "2020-06-24 11:39:46,783 epoch 17 - iter 40/40 - loss 15.43464363 - samples/sec: 80.93\n",
      "2020-06-24 11:39:46,793 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:46,793 EPOCH 17 done: loss 15.4346 - lr 0.1000000\n",
      "2020-06-24 11:39:47,319 DEV : loss 7.4856719970703125 - score 0.7847\n",
      "2020-06-24 11:39:47,327 BAD EPOCHS (no improvement): 1\n",
      "2020-06-24 11:39:47,328 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:39:48,512 epoch 18 - iter 4/40 - loss 15.76224136 - samples/sec: 108.25\n",
      "2020-06-24 11:39:49,669 epoch 18 - iter 8/40 - loss 15.23633206 - samples/sec: 111.68\n",
      "2020-06-24 11:39:51,219 epoch 18 - iter 12/40 - loss 15.60923886 - samples/sec: 83.19\n",
      "2020-06-24 11:39:52,508 epoch 18 - iter 16/40 - loss 15.54774988 - samples/sec: 100.15\n",
      "2020-06-24 11:39:53,678 epoch 18 - iter 20/40 - loss 15.62653170 - samples/sec: 110.25\n",
      "2020-06-24 11:39:55,155 epoch 18 - iter 24/40 - loss 15.57776968 - samples/sec: 87.36\n",
      "2020-06-24 11:39:56,766 epoch 18 - iter 28/40 - loss 15.40228530 - samples/sec: 79.97\n",
      "2020-06-24 11:39:58,171 epoch 18 - iter 32/40 - loss 15.52539176 - samples/sec: 91.76\n",
      "2020-06-24 11:39:59,891 epoch 18 - iter 36/40 - loss 15.36939592 - samples/sec: 74.88\n",
      "2020-06-24 11:40:01,251 epoch 18 - iter 40/40 - loss 15.07363307 - samples/sec: 94.73\n",
      "2020-06-24 11:40:01,262 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:01,263 EPOCH 18 done: loss 15.0736 - lr 0.1000000\n",
      "2020-06-24 11:40:01,787 DEV : loss 7.108133792877197 - score 0.8006\n",
      "2020-06-24 11:40:01,794 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:40:05,472 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:07,368 epoch 19 - iter 4/40 - loss 16.32384944 - samples/sec: 67.62\n",
      "2020-06-24 11:40:08,485 epoch 19 - iter 8/40 - loss 15.30355632 - samples/sec: 115.85\n",
      "2020-06-24 11:40:09,557 epoch 19 - iter 12/40 - loss 14.95245067 - samples/sec: 120.59\n",
      "2020-06-24 11:40:10,790 epoch 19 - iter 16/40 - loss 14.63557512 - samples/sec: 104.67\n",
      "2020-06-24 11:40:11,820 epoch 19 - iter 20/40 - loss 14.59729009 - samples/sec: 125.69\n",
      "2020-06-24 11:40:12,844 epoch 19 - iter 24/40 - loss 14.51631761 - samples/sec: 126.00\n",
      "2020-06-24 11:40:14,289 epoch 19 - iter 28/40 - loss 14.90180683 - samples/sec: 89.24\n",
      "2020-06-24 11:40:15,637 epoch 19 - iter 32/40 - loss 14.99209040 - samples/sec: 95.59\n",
      "2020-06-24 11:40:16,834 epoch 19 - iter 36/40 - loss 14.91162713 - samples/sec: 107.87\n",
      "2020-06-24 11:40:17,681 epoch 19 - iter 40/40 - loss 14.60128539 - samples/sec: 152.93\n",
      "2020-06-24 11:40:17,691 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:17,692 EPOCH 19 done: loss 14.6013 - lr 0.1000000\n",
      "2020-06-24 11:40:18,169 DEV : loss 6.840267658233643 - score 0.8095\n",
      "2020-06-24 11:40:18,177 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:40:21,661 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:23,069 epoch 20 - iter 4/40 - loss 15.33908129 - samples/sec: 91.10\n",
      "2020-06-24 11:40:24,114 epoch 20 - iter 8/40 - loss 14.02811408 - samples/sec: 123.49\n",
      "2020-06-24 11:40:25,109 epoch 20 - iter 12/40 - loss 13.98810093 - samples/sec: 130.04\n",
      "2020-06-24 11:40:26,198 epoch 20 - iter 16/40 - loss 14.08040327 - samples/sec: 118.65\n",
      "2020-06-24 11:40:27,646 epoch 20 - iter 20/40 - loss 13.98449764 - samples/sec: 89.02\n",
      "2020-06-24 11:40:28,753 epoch 20 - iter 24/40 - loss 14.14428174 - samples/sec: 116.52\n",
      "2020-06-24 11:40:29,742 epoch 20 - iter 28/40 - loss 14.09144878 - samples/sec: 130.83\n",
      "2020-06-24 11:40:31,082 epoch 20 - iter 32/40 - loss 14.16546193 - samples/sec: 96.38\n",
      "2020-06-24 11:40:32,357 epoch 20 - iter 36/40 - loss 14.31697000 - samples/sec: 101.09\n",
      "2020-06-24 11:40:33,355 epoch 20 - iter 40/40 - loss 14.17440026 - samples/sec: 129.29\n",
      "2020-06-24 11:40:33,366 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:33,367 EPOCH 20 done: loss 14.1744 - lr 0.1000000\n",
      "2020-06-24 11:40:33,834 DEV : loss 6.793857097625732 - score 0.8122\n",
      "2020-06-24 11:40:33,841 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:40:37,336 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:38,477 epoch 21 - iter 4/40 - loss 14.57898331 - samples/sec: 112.45\n",
      "2020-06-24 11:40:39,317 epoch 21 - iter 8/40 - loss 12.97333801 - samples/sec: 154.24\n",
      "2020-06-24 11:40:40,677 epoch 21 - iter 12/40 - loss 13.46717374 - samples/sec: 94.83\n",
      "2020-06-24 11:40:41,572 epoch 21 - iter 16/40 - loss 13.29634941 - samples/sec: 144.76\n",
      "2020-06-24 11:40:42,708 epoch 21 - iter 20/40 - loss 13.44047027 - samples/sec: 113.61\n",
      "2020-06-24 11:40:43,847 epoch 21 - iter 24/40 - loss 13.31191317 - samples/sec: 113.36\n",
      "2020-06-24 11:40:45,344 epoch 21 - iter 28/40 - loss 13.57347434 - samples/sec: 86.18\n",
      "2020-06-24 11:40:46,872 epoch 21 - iter 32/40 - loss 14.14517495 - samples/sec: 84.49\n",
      "2020-06-24 11:40:47,962 epoch 21 - iter 36/40 - loss 14.04460650 - samples/sec: 118.54\n",
      "2020-06-24 11:40:49,034 epoch 21 - iter 40/40 - loss 14.09167178 - samples/sec: 120.70\n",
      "2020-06-24 11:40:49,046 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:49,047 EPOCH 21 done: loss 14.0917 - lr 0.1000000\n",
      "2020-06-24 11:40:49,527 DEV : loss 6.490757465362549 - score 0.816\n",
      "2020-06-24 11:40:49,535 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:40:53,137 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:40:54,094 epoch 22 - iter 4/40 - loss 13.73776722 - samples/sec: 133.86\n",
      "2020-06-24 11:40:55,342 epoch 22 - iter 8/40 - loss 13.99218822 - samples/sec: 103.48\n",
      "2020-06-24 11:40:56,481 epoch 22 - iter 12/40 - loss 13.78356059 - samples/sec: 113.49\n",
      "2020-06-24 11:40:57,551 epoch 22 - iter 16/40 - loss 13.36897022 - samples/sec: 120.88\n",
      "2020-06-24 11:40:58,922 epoch 22 - iter 20/40 - loss 13.57842698 - samples/sec: 94.11\n",
      "2020-06-24 11:41:00,036 epoch 22 - iter 24/40 - loss 13.48381150 - samples/sec: 115.82\n",
      "2020-06-24 11:41:01,339 epoch 22 - iter 28/40 - loss 13.64853164 - samples/sec: 99.08\n",
      "2020-06-24 11:41:02,663 epoch 22 - iter 32/40 - loss 13.69846252 - samples/sec: 97.45\n",
      "2020-06-24 11:41:03,871 epoch 22 - iter 36/40 - loss 13.68254203 - samples/sec: 106.71\n",
      "2020-06-24 11:41:04,917 epoch 22 - iter 40/40 - loss 13.51532786 - samples/sec: 123.42\n",
      "2020-06-24 11:41:04,925 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:04,926 EPOCH 22 done: loss 13.5153 - lr 0.1000000\n",
      "2020-06-24 11:41:05,388 DEV : loss 6.306644916534424 - score 0.822\n",
      "2020-06-24 11:41:05,396 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-06-24 11:41:08,932 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:10,479 epoch 23 - iter 4/40 - loss 14.39613152 - samples/sec: 82.93\n",
      "2020-06-24 11:41:11,598 epoch 23 - iter 8/40 - loss 13.71122885 - samples/sec: 115.38\n",
      "2020-06-24 11:41:12,787 epoch 23 - iter 12/40 - loss 14.27119557 - samples/sec: 108.55\n",
      "2020-06-24 11:41:13,759 epoch 23 - iter 16/40 - loss 13.54640162 - samples/sec: 132.95\n",
      "2020-06-24 11:41:15,033 epoch 23 - iter 20/40 - loss 13.59998627 - samples/sec: 101.37\n",
      "2020-06-24 11:41:16,397 epoch 23 - iter 24/40 - loss 13.49087167 - samples/sec: 94.57\n",
      "2020-06-24 11:41:17,508 epoch 23 - iter 28/40 - loss 13.53594886 - samples/sec: 116.30\n",
      "2020-06-24 11:41:18,622 epoch 23 - iter 32/40 - loss 13.53190812 - samples/sec: 116.11\n",
      "2020-06-24 11:41:19,906 epoch 23 - iter 36/40 - loss 13.53802114 - samples/sec: 100.43\n",
      "2020-06-24 11:41:21,042 epoch 23 - iter 40/40 - loss 13.75264115 - samples/sec: 113.87\n",
      "2020-06-24 11:41:21,050 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:21,051 EPOCH 23 done: loss 13.7526 - lr 0.1000000\n",
      "2020-06-24 11:41:21,520 DEV : loss 6.401901721954346 - score 0.819\n",
      "2020-06-24 11:41:21,528 BAD EPOCHS (no improvement): 1\n",
      "2020-06-24 11:41:21,529 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:22,677 epoch 24 - iter 4/40 - loss 12.04440808 - samples/sec: 111.59\n",
      "2020-06-24 11:41:23,747 epoch 24 - iter 8/40 - loss 13.06927240 - samples/sec: 120.56\n",
      "2020-06-24 11:41:24,806 epoch 24 - iter 12/40 - loss 13.03706646 - samples/sec: 122.14\n",
      "2020-06-24 11:41:26,335 epoch 24 - iter 16/40 - loss 13.32810080 - samples/sec: 84.19\n",
      "2020-06-24 11:41:27,254 epoch 24 - iter 20/40 - loss 12.83870311 - samples/sec: 141.07\n",
      "2020-06-24 11:41:28,417 epoch 24 - iter 24/40 - loss 13.18581482 - samples/sec: 110.94\n",
      "2020-06-24 11:41:29,836 epoch 24 - iter 28/40 - loss 13.28393190 - samples/sec: 90.89\n",
      "2020-06-24 11:41:31,063 epoch 24 - iter 32/40 - loss 13.48773193 - samples/sec: 105.15\n",
      "2020-06-24 11:41:32,237 epoch 24 - iter 36/40 - loss 13.30179074 - samples/sec: 109.88\n",
      "2020-06-24 11:41:33,554 epoch 24 - iter 40/40 - loss 13.27754097 - samples/sec: 98.08\n",
      "2020-06-24 11:41:33,564 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:33,565 EPOCH 24 done: loss 13.2775 - lr 0.1000000\n",
      "2020-06-24 11:41:34,041 DEV : loss 6.42360782623291 - score 0.8132\n",
      "2020-06-24 11:41:34,048 BAD EPOCHS (no improvement): 2\n",
      "2020-06-24 11:41:34,049 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:34,658 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:34,659 Exiting from training early.\n",
      "2020-06-24 11:41:34,660 Saving model ...\n",
      "2020-06-24 11:41:37,747 Done.\n",
      "2020-06-24 11:41:37,748 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-24 11:41:37,748 Testing using best model ...\n",
      "2020-06-24 11:41:37,749 loading file resources/taggers/example-pos/best-model.pt\n",
      "2020-06-24 11:41:38,982 0.8341\t0.8351\t0.8346\n",
      "2020-06-24 11:41:38,983 \n",
      "MICRO_AVG: acc 0.8347 - f1-score 0.8346\n",
      "MACRO_AVG: acc 0.6487 - f1-score 0.6471\n",
      "$          tp: 5 - fp: 0 - fn: 0 - tn: 5 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "''         tp: 10 - fp: 0 - fn: 1 - tn: 8 - precision: 1.0000 - recall: 0.9091 - accuracy: 0.9474 - f1-score: 0.9524\n",
      ",          tp: 104 - fp: 1 - fn: 5 - tn: 107 - precision: 0.9905 - recall: 0.9541 - accuracy: 0.9724 - f1-score: 0.9720\n",
      "-LRB-      tp: 8 - fp: 5 - fn: 1 - tn: 8 - precision: 0.6154 - recall: 0.8889 - accuracy: 0.7273 - f1-score: 0.7273\n",
      "-RRB-      tp: 8 - fp: 4 - fn: 1 - tn: 8 - precision: 0.6667 - recall: 0.8889 - accuracy: 0.7619 - f1-score: 0.7619\n",
      ".          tp: 155 - fp: 10 - fn: 3 - tn: 155 - precision: 0.9394 - recall: 0.9810 - accuracy: 0.9598 - f1-score: 0.9598\n",
      ":          tp: 3 - fp: 0 - fn: 0 - tn: 3 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "ADD        tp: 0 - fp: 1 - fn: 6 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "CC         tp: 81 - fp: 1 - fn: 4 - tn: 81 - precision: 0.9878 - recall: 0.9529 - accuracy: 0.9701 - f1-score: 0.9701\n",
      "CD         tp: 30 - fp: 9 - fn: 8 - tn: 30 - precision: 0.7692 - recall: 0.7895 - accuracy: 0.7792 - f1-score: 0.7792\n",
      "DT         tp: 204 - fp: 12 - fn: 7 - tn: 204 - precision: 0.9444 - recall: 0.9668 - accuracy: 0.9555 - f1-score: 0.9555\n",
      "EX         tp: 1 - fp: 0 - fn: 1 - tn: 1 - precision: 1.0000 - recall: 0.5000 - accuracy: 0.6667 - f1-score: 0.6667\n",
      "FW         tp: 0 - fp: 0 - fn: 1 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "GW         tp: 0 - fp: 0 - fn: 2 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "HYPH       tp: 4 - fp: 5 - fn: 0 - tn: 4 - precision: 0.4444 - recall: 1.0000 - accuracy: 0.6154 - f1-score: 0.6154\n",
      "IN         tp: 223 - fp: 25 - fn: 16 - tn: 222 - precision: 0.8992 - recall: 0.9331 - accuracy: 0.9156 - f1-score: 0.9158\n",
      "JJ         tp: 113 - fp: 68 - fn: 38 - tn: 115 - precision: 0.6243 - recall: 0.7483 - accuracy: 0.6826 - f1-score: 0.6807\n",
      "JJR        tp: 0 - fp: 0 - fn: 5 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "JJS        tp: 5 - fp: 1 - fn: 5 - tn: 5 - precision: 0.8333 - recall: 0.5000 - accuracy: 0.6250 - f1-score: 0.6250\n",
      "MD         tp: 34 - fp: 1 - fn: 1 - tn: 34 - precision: 0.9714 - recall: 0.9714 - accuracy: 0.9714 - f1-score: 0.9714\n",
      "NFP        tp: 1 - fp: 1 - fn: 7 - tn: 2 - precision: 0.5000 - recall: 0.1250 - accuracy: 0.2727 - f1-score: 0.2000\n",
      "NN         tp: 301 - fp: 134 - fn: 40 - tn: 301 - precision: 0.6920 - recall: 0.8827 - accuracy: 0.7758 - f1-score: 0.7758\n",
      "NNP        tp: 92 - fp: 21 - fn: 68 - tn: 92 - precision: 0.8142 - recall: 0.5750 - accuracy: 0.6740 - f1-score: 0.6740\n",
      "NNPS       tp: 0 - fp: 0 - fn: 3 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "NNS        tp: 36 - fp: 12 - fn: 32 - tn: 36 - precision: 0.7500 - recall: 0.5294 - accuracy: 0.6207 - f1-score: 0.6207\n",
      "PDT        tp: 0 - fp: 0 - fn: 3 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "POS        tp: 7 - fp: 0 - fn: 1 - tn: 7 - precision: 1.0000 - recall: 0.8750 - accuracy: 0.9333 - f1-score: 0.9333\n",
      "PRP        tp: 142 - fp: 6 - fn: 3 - tn: 142 - precision: 0.9595 - recall: 0.9793 - accuracy: 0.9693 - f1-score: 0.9693\n",
      "PRP$       tp: 32 - fp: 1 - fn: 0 - tn: 32 - precision: 0.9697 - recall: 1.0000 - accuracy: 0.9846 - f1-score: 0.9846\n",
      "RB         tp: 93 - fp: 23 - fn: 43 - tn: 95 - precision: 0.8017 - recall: 0.6838 - accuracy: 0.7402 - f1-score: 0.7381\n",
      "RBR        tp: 1 - fp: 1 - fn: 0 - tn: 1 - precision: 0.5000 - recall: 1.0000 - accuracy: 0.6667 - f1-score: 0.6667\n",
      "RBS        tp: 0 - fp: 0 - fn: 3 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "RP         tp: 9 - fp: 9 - fn: 5 - tn: 9 - precision: 0.5000 - recall: 0.6429 - accuracy: 0.5625 - f1-score: 0.5625\n",
      "SYM        tp: 0 - fp: 0 - fn: 2 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "TO         tp: 40 - fp: 4 - fn: 1 - tn: 39 - precision: 0.9091 - recall: 0.9756 - accuracy: 0.9405 - f1-score: 0.9412\n",
      "UH         tp: 8 - fp: 3 - fn: 7 - tn: 8 - precision: 0.7273 - recall: 0.5333 - accuracy: 0.6154 - f1-score: 0.6154\n",
      "VB         tp: 101 - fp: 12 - fn: 24 - tn: 101 - precision: 0.8938 - recall: 0.8080 - accuracy: 0.8487 - f1-score: 0.8487\n",
      "VBD        tp: 43 - fp: 6 - fn: 7 - tn: 43 - precision: 0.8776 - recall: 0.8600 - accuracy: 0.8687 - f1-score: 0.8687\n",
      "VBG        tp: 19 - fp: 4 - fn: 11 - tn: 20 - precision: 0.8261 - recall: 0.6333 - accuracy: 0.7222 - f1-score: 0.7170\n",
      "VBN        tp: 37 - fp: 11 - fn: 18 - tn: 37 - precision: 0.7708 - recall: 0.6727 - accuracy: 0.7184 - f1-score: 0.7184\n",
      "VBP        tp: 60 - fp: 17 - fn: 6 - tn: 59 - precision: 0.7792 - recall: 0.9091 - accuracy: 0.8380 - f1-score: 0.8392\n",
      "VBZ        tp: 64 - fp: 1 - fn: 8 - tn: 64 - precision: 0.9846 - recall: 0.8889 - accuracy: 0.9343 - f1-score: 0.9343\n",
      "WDT        tp: 7 - fp: 1 - fn: 6 - tn: 7 - precision: 0.8750 - recall: 0.5385 - accuracy: 0.6667 - f1-score: 0.6667\n",
      "WP         tp: 6 - fp: 3 - fn: 6 - tn: 6 - precision: 0.6667 - recall: 0.5000 - accuracy: 0.5714 - f1-score: 0.5714\n",
      "WRB        tp: 4 - fp: 4 - fn: 4 - tn: 4 - precision: 0.5000 - recall: 0.5000 - accuracy: 0.5000 - f1-score: 0.5000\n",
      "``         tp: 10 - fp: 1 - fn: 2 - tn: 9 - precision: 0.9091 - recall: 0.8333 - accuracy: 0.8636 - f1-score: 0.8696\n",
      "2020-06-24 11:41:38,984 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.834558093346574,\n",
       " 'dev_score_history': [0.26642501132759405,\n",
       "  0.4461538461538461,\n",
       "  0.539768864717879,\n",
       "  0.6363636363636364,\n",
       "  0.6601853945286005,\n",
       "  0.6957110609480812,\n",
       "  0.7068654019873533,\n",
       "  0.7168960072185879,\n",
       "  0.7465583389754006,\n",
       "  0.753780185059806,\n",
       "  0.7657555906934719,\n",
       "  0.778505305938135,\n",
       "  0.780388612742883,\n",
       "  0.7717317678934297,\n",
       "  0.7920523820275457,\n",
       "  0.7978314885927265,\n",
       "  0.7847300655071154,\n",
       "  0.8006314839873703,\n",
       "  0.8094808126410835,\n",
       "  0.8121896162528216,\n",
       "  0.8159855497855046,\n",
       "  0.8220415537488708,\n",
       "  0.8189616252821671,\n",
       "  0.8131917777275808],\n",
       " 'train_loss_history': [53.158365631103514,\n",
       "  40.7812376499176,\n",
       "  33.89535856246948,\n",
       "  28.774631690979003,\n",
       "  25.575098514556885,\n",
       "  23.40834789276123,\n",
       "  21.724063682556153,\n",
       "  20.738570284843444,\n",
       "  19.54983322620392,\n",
       "  19.030347990989686,\n",
       "  18.23484809398651,\n",
       "  17.77282521724701,\n",
       "  16.75617399215698,\n",
       "  16.35137629508972,\n",
       "  16.097274255752563,\n",
       "  15.75730516910553,\n",
       "  15.434643626213074,\n",
       "  15.073633074760437,\n",
       "  14.601285386085511,\n",
       "  14.17440025806427,\n",
       "  14.091671776771545,\n",
       "  13.515327858924866,\n",
       "  13.75264115333557,\n",
       "  13.277540969848634],\n",
       " 'dev_loss_history': [29.557079315185547,\n",
       "  22.730993270874023,\n",
       "  18.454727172851562,\n",
       "  14.629263877868652,\n",
       "  13.127612113952637,\n",
       "  11.939422607421875,\n",
       "  11.031184196472168,\n",
       "  10.523231506347656,\n",
       "  9.60346794128418,\n",
       "  9.251897811889648,\n",
       "  8.846505165100098,\n",
       "  8.3197021484375,\n",
       "  8.148077964782715,\n",
       "  8.169198036193848,\n",
       "  7.650144100189209,\n",
       "  7.284870147705078,\n",
       "  7.4856719970703125,\n",
       "  7.108133792877197,\n",
       "  6.840267658233643,\n",
       "  6.793857097625732,\n",
       "  6.490757465362549,\n",
       "  6.306644916534424,\n",
       "  6.401901721954346,\n",
       "  6.42360782623291]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = UD_ENGLISH().downsample(0.1)\n",
    "print(corpus)\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'pos'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-pos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
