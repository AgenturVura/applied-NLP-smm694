{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Week 2\n",
    "======\n",
    "\n",
    "― Representing words and meanings\n",
    "------------------------------------------------------\n",
    "\n",
    "― Language modeling\n",
    "--------------------------------\n",
    "\n",
    "<img src=\"images/_99.jpg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Meanings are central to natural language \n",
    "================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Key **emerging properties** of natural language:\n",
    "\n",
    "+ language is socially-oriented\n",
    "+ words reflect symbols and social categories (that is, culture)\n",
    "+ language convey (ambiguous) meanings\n",
    "\n",
    "The **concept of meaning** is the place to start for any natural language processing analyses.\n",
    "\n",
    "Let's have a closer look at:\n",
    "\n",
    "+ how 'meanings' are represented in computational linguistics\n",
    "+ how machines look at 'meanings'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_0.jpg\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to represent the meaning of a word?\n",
    "=====================================\n",
    "\n",
    "A (computational) linguist's perspective\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Two pillars that reflect how linguists' think about meanings:\n",
    "\n",
    "+ denotational semantics\n",
    "+ distributional hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The intuition behind denotational semantics\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Semantics, as the study of meanings, concerns the relationship between signifiers ― like words, phrases, signs, and symbols ― and what they stand for in reality, their denotation.\n",
    "\n",
    "Denotations comprise both the salient features associated with an entity (being a concrete instance or a category) and the cognitive and behavioral effects of using a signifier that invokes an entity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example: the lexeme 'hip-hop' conveys meanings about what constitute a 'hip-hop' song as well as the values, norms, and beliefs that orient the behavior of 'hip-hop people.'\n",
    "\n",
    "<img src=\"images/_2.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The distributional hypothesis (DH)\n",
    "============================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*''Difference of meaning correlates with difference of distribution''*\n",
    "\n",
    "―Harris, 1954\n",
    "\n",
    "*''Semantic similarity is a function of the contexts in which words are used.''*\n",
    "\n",
    "―Miller & Charles, 1951\n",
    "\n",
    "*''DS is not only a method for lexical analysis but also a theoretical framework to build computational models of semantic memory''*\n",
    "\n",
    "―Lenci, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DH lies at the hearth of vector space models.\n",
    "\n",
    "<img src=\"images/_1.png\" width=\"100%\">\n",
    "\n",
    "Fig. 1 ― Distributional vectors of the lexemes car, cat, dog, and van. Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distributional representations\n",
    "========================\n",
    "\n",
    "The distributional representation of a lexical item is typically a distributional vector representing its co-occurrences with linguistic contexts ― hence the name vector space semantics.\n",
    "\n",
    "The kind of co-occurrence relation between target and context lexemes determines different\n",
    "types of collocates and distributional representations.\n",
    "\n",
    "Context types (Firth (1957): (You shall know a word) by the company it keeps!)\n",
    "\n",
    "| Context types                                | Co-occurrences             |\n",
    "| -------------------------------------------- | -------------------------- |\n",
    "| Undirected window-based collocate            | $word$                     |\n",
    "| Directed window-based collocate              | $\\langle R, word \\rangle$  |\n",
    "| [Dependency-filtered syntactic collocate][1] | word                       |\n",
    "| Dependency-typed syntactic collocate         | $\\langle obj, word \\rangle$|\n",
    "| Text region                                  | Firth (1957)               |\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'\n",
    "\n",
    "    [1]: https://spacy.io/displacy-3504502e1d5463ede765f0a789717424.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building distributional representations (1/3)\n",
    "====================================\n",
    "\n",
    "The basic method of building distributional vectors consists of the following procedure:\n",
    "\n",
    "+ co-occurrences between lexical items and linguistic contexts are extracted from a corpus and counted\n",
    "+ the distribution of lexical items is represented with a co-occurrence matrix, whose rows correspond to target lexical items, columns to contexts, and the entries to their co-occurrence frequency\n",
    "+ raw frequencies are then usually transformed into significance weights to reflect the importance of the contexts\n",
    "+ the semantic similarity between lexemes is measured with the similarity between their row vectors in the co-occurrence matrix\n",
    "\n",
    "Suppose we have extracted and counted the co-occurrences of the targets $T =\\{bike, car, dog, lion\\}$ with the context lexemes $C =\\{bite, buy, drive, eat, get, live, park, ride, tell\\}$ in a corpus. Their distribution is represented with the following co-occurrence matrix $MT x C$,in which mt,c is the co-occurrence frequency of t with $c$:\n",
    "\n",
    "<img src=\"images/_4.png\" width=\"70%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building distributional representations (2/3)\n",
    "====================================\n",
    "\n",
    "The most common weighting function in DS is positive pointwise mutual information (PPMI) (Bullinaria & Levy 2007).\n",
    "\n",
    "PPMI measures how much the probability of a target–context pair estimated in the training corpus is higher than the probability we should expect if the target and the context occurred independently of one another.\n",
    "\n",
    "Matrix 3 contains the PPMI weights computed from the raw co- occurrence frequencies in matrix 1\n",
    "\n",
    "\\begin{equation}\n",
    "PPMI(t,c) = max \\bigg( 0, log_{2} \\frac{p(t,c)}{p(t)p(c)} \\bigg )\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"images/_5.png\" width=\"70%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building distributional representations (3/3)\n",
    "====================================\n",
    "\n",
    "The distributional similarity between two lexemes u and v is measured with the similarity\n",
    "between their distributional vectors u and v.\n",
    "\n",
    "Once we have computed the pairwise distributional similarity between the targets, we can identify the k nearest neighbors of each target t, that is, the k lexical items with the highest similarity score with t. The cosine is the most popular measure of vector similarity in DS:\n",
    "\n",
    "\\begin{equation}\n",
    "cos(u,v) = \\frac{u \\cdot v}{\\Vert u \\Vert \\Vert v \\Vert} \n",
    "\\end{equation}\n",
    "\n",
    "The cosine ranges from 1 for identical vectors to −1 (0, if the vectors do not contain negative values).matrix reports the cosines between the row vectors in matrix 3:\n",
    "\n",
    "<img src=\"images/_6.png\" width=\"35%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distributional semantics and NLP frameworks/tools\n",
    "==========================================\n",
    "\n",
    "<img src=\"images/_7.png\" width=\"90%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to represent the meaning of a word?\n",
    "=====================================\n",
    "\n",
    "A machine's perspective\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Human beings are entrenched in the symbols and social categories of natural language, while machines are not. Hence, machines are not able to associate meanings with lexemes.\n",
    "\n",
    "This explains why analyzing massive datasets of natural language has been traditionally taxing/impossible. In fact, human beings are really good at making sense of language but they are bad at computing (so, hand-curated work-flows are not scalable). On the contrary, machines are really good at computing but they're just dull (so, computation capacity looks for a work-flow to scale-up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mainly, there are two strategies through which machines can handle meanings:\n",
    "\n",
    "+ human beings can provide machines with 'pattern-matching' rules that induce meaningful responses vis a' vis natural language inputs\n",
    "+ with the aid of statistical frameworks (e.g. Distributional Representations), machines can discover/learn the  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pattern-matching route\n",
    "===================\n",
    "\n",
    "Two prominent natural language tools tools that draw on pattern matching:\n",
    "\n",
    "+ regular expressions\n",
    "+ WordNet (an example of annotated dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regular expressions\n",
    "================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regular expressions use a special kind (class) of formal language grammar called a regular grammar.\n",
    "\n",
    "Regular grammars have predictable, provable behavior, and yet are flexible enough to power some of the most sophisticated dialog engines and chatbots on the market. Amazon Alexa and Google Now are mostly pattern-based engines that rely on regular grammars.\n",
    "\n",
    "Deep, complex regular grammar rules can often be expressed in a single line of code called a regular expression. There are successful chatbot frameworks in Python, like Will, that rely exclusively on this kind of language to produce some useful and interesting behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_9.png\" width=\"100%\">\n",
    "\n",
    "Examples of home assistant products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regular expressions: A minimal chatbot (1/3)\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "m0 : <re.Match object; span=(0, 10), match='Hello Rosa'>\n",
      "\n",
      "m1 : <re.Match object; span=(0, 5), match='hi ho'>\n",
      "\n",
      "m2 : <re.Match object; span=(0, 3), match='hey'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Credits to Lane, Howard & Hapke (2019)\n",
    "'''\n",
    "\n",
    "# load re module\n",
    "import re\n",
    "\n",
    "# greeting matcher\n",
    "r = \"(hi|hello|hey)[ ]*([a-z]*)\"\n",
    "\n",
    "# matcher in action\n",
    "m0 = re.match(r, 'Hello Rosa', flags=re.IGNORECASE)\n",
    "m1 = re.match(r, \"hi ho, hi ho, it's off to work ...\", flags=re.IGNORECASE)\n",
    "m2 = re.match(r, \"hey, what's up\", flags=re.IGNORECASE)\n",
    "\n",
    "print(\"\"\"\n",
    "m0 : {}\n",
    "\n",
    "m1 : {}\n",
    "\n",
    "m2 : {}\n",
    "\"\"\".format(m0, m1, m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regular expressions: A minimal chatbot (2/3)\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's expand the greeting matcher\n",
    "r = r\"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|\"\\\n",
    "    r\"afternoon|even[gin']{0,3}))[\\s,;:]{1,3}([a-z]{1,20})\"\n",
    "\n",
    "# ... and ignore the case of text\n",
    "re_greeting = re.compile(r, flags=re.IGNORECASE)\n",
    "\n",
    "# matcher in action (uncomment the below to run)\n",
    "# re_greeting.match('Hello Rosa')\n",
    "# re_greeting.match('Hello Rosa').groups()\n",
    "# re_greeting.match(\"Good morning Rosa\")\n",
    "# re_greeting.match(\"Good Manning Rosa\")\n",
    "# re_greeting.match('Good evening Rosa Parks').groups() \n",
    "# re_greeting.match(\"Good Morn'n Rosa\")\n",
    "# re_greeting.match(\"yo Rosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regular expressions: A minimal chatbot (3/3)\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set of name for the bot\n",
    "my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot', 'chatterbot'])\n",
    "\n",
    "# possible curt names to use in the conversation\n",
    "curt_names = set(['hal', 'you', 'u'])\n",
    "\n",
    "# name of the conversant (we pretend to know her/him)\n",
    "greeter_name = 'Simone'\n",
    "\n",
    "# let's recycle the matcher\n",
    "match = re_greeting.match(input())\n",
    "\n",
    "# conditional statment that initiates the conversation (run and populate)\n",
    "if match:\n",
    "    at_name = match.groups()[-1]\n",
    "    if at_name in curt_names:\n",
    "        print(\"Good one.\")\n",
    "    elif at_name.lower() in my_names:\n",
    "        print(\"Hi {}, How are you?\".format(greeter_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wordnet\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[WordNet®](https://wordnet.princeton.edu/) is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations.\n",
    "\n",
    "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, Wordnet presents some key nuances (see graph on the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_11.png\" width=\"100%\">\n",
    "\n",
    "[Fragment of WordNet Concept Hierarchy](http://nltk.sourceforge.net/doc/en/ch01.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The anatomy of WordNet\n",
    "====================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Structure**\n",
    "\n",
    "The main relation among words in WordNet is synonymy, as between the words shut and close or car and automobile. Synonyms ― words that denote the same concept and are interchangeable in many contexts ― are grouped into unordered sets (synsets).\n",
    "\n",
    "Each of WordNet’s 117 000 synsets is linked to other synsets by means of a small number of 'conceptual relations.' \n",
    "\n",
    "Additionally, a synset contains a brief definition ('gloss') and, in most cases, one or more short sentences illustrating the use of the synset members.\n",
    "\n",
    "Word forms with several distinct meanings are represented in as many distinct synsets. Thus, each form-meaning pair in WordNet is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Relations**\n",
    "\n",
    "The most frequently encoded relation among synsets is the super-subordinate relation (also called hyperonymy, hyponymy or ISA relation).\n",
    "\n",
    "It links more general synsets like $\\texttt{furniture}$ or $\\texttt{piece_of_furniture}$ to increasingly specific ones like $\\texttt{bed}$ and $\\texttt{bunkbed}$.\n",
    "\n",
    "Thus, WordNet states that the category furniture includes bed, which in turn includes bunkbed; conversely, concepts like bed and bunkbed make up the category furniture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "WordNet in action: synonims\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's import wordnet from nltk.corpus\n",
    "\n",
    "this assumes that you've downloaded the 'wordnet' corpus before\n",
    "\n",
    "if not, you can do that with:\n",
    "\n",
    "nltk.download('wordnet')\n",
    "'''\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "'''\n",
    "# the tags adopted by nltk are not very informative\n",
    "# let's make them self-explanatory\n",
    "'''\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "\n",
    "for synset in wn.synsets(\"fine\"):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()], \n",
    "                          \", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "WordNet in action: hyperonimy relationship\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset(\"python.n.1\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, is WordNet enough to do NLP?\n",
    "============================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROS**\n",
    "\n",
    "+ it bring usable meanings to machines (e.g., it can inform our chat-bot)\n",
    "+ great as a resource for research & teaching "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**CONS**\n",
    "\n",
    "+ bottleneck: WordNet is an annotated dataset\n",
    "+ it requires human labor to adapt\n",
    "  - perhaps, it's impossible to keep-up-to date\n",
    "  - at least, it misses new meanings of words\n",
    "+ it is subjective\n",
    "+ missing nuance (Manning, 2019): e.g. 'proficient' is listed as a synonym for 'good'.\n",
    "+ it doesn't offer a continuous measure of word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is there a statistical or machine learning approach that might work in place of the pattern-matching approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Provided we have access to a reasonably large (and diverse) corpus of text, how can we represent the duality between words and meanings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Statistical framework route\n",
    "======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Traditional NLP\n",
    "    - Bag-of-Words\n",
    "    - One-hot encodings\n",
    "\n",
    "+ Modern NLP\n",
    "    - Embeddings (e.g., a vector space generated via $\\texttt{word2vec}$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_8.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bag-of-Words (BoW)\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given some text corpora $D$, a BoW work-flow implies the following steps:\n",
    "\n",
    "1. $\\forall d \\in D$ (e.g, documents, state- ments, sentences, and even single words), get the the token $\\Phi(d)$\n",
    "2. $\\forall s \\in S$ (i.e., unique lexical items) and $\\forall d \\in D$, get the cardinality $\\vert s \\vert$\n",
    "\n",
    "We call the possible vectors a machine might create this way a vector space.\n",
    "\n",
    "Such a vector space allows us to use linear algebra (and libraries such as NumPy, Scipy, or Numba) to manipulate lexical items and compute things like distances and statistics involving natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_12.png\" width=\"100%\">\n",
    "\n",
    "Token sorting tray (source is Lane, Howard & Hapke 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What can we do with BoW data?\n",
    "==========================\n",
    "\n",
    "For example, we can address search queries such as: *''What is the combination of words most likely to follow a particular bag of words?''* Or, if a user enters a sequence of words, *''What is the closest bag of words in our database to a bag-of-words vector provided by the user?''*\n",
    "\n",
    "General take-home on BoW:\n",
    "\n",
    "+ a BoW approach can generate meaningful responses to answers \n",
    "+ in a BoW approach, humans do not pass any rules to machines (you remember pattern-matching?)\n",
    "+ BoW leverages distributional data to appreciate the semantic similarity of lexical items\n",
    "\n",
    "Warning: a BoW approach doesn't say anything about the specific meanings of lexical items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BoW in action\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's import Counter, a special kind of\n",
    "# dictionary that computes the cardinality of\n",
    "# elements\n",
    "from collections import Counter\n",
    "\n",
    "# sample sentence\n",
    "sentence = \"\"\"\n",
    "Success is not final; failure is not fatal: It is the courage to continue that counts. -Winston S. Churchill\n",
    "\"\"\"\n",
    "\n",
    "# tokenization\n",
    "tokens = sentence.split()\n",
    "\n",
    "# BoW\n",
    "bow = Counter(tokens)\n",
    "\n",
    "# print\n",
    "from pprint import pprint\n",
    "pprint(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One-hot vectors \n",
    "=============\n",
    "\n",
    "One of the main limitations of the BoW approach is the proliferation of unique vectors to compare and contrast.\n",
    "\n",
    "One-hot vectors (a form of discrete representation of lexical items) mitigate the curse of dimensionality by considering whether a word is or is not present in a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's import numpy to manipulat the text\n",
    "import numpy as np\n",
    "\n",
    "# sample sentence\n",
    "sentence = \"\"\"\n",
    "Success is not final; failure is not fatal: It is the courage to continue that counts. -Winston S. Churchill\n",
    "\"\"\"\n",
    "\n",
    "# tokenization\n",
    "tokens = str.split(sentence)\n",
    "\n",
    "# vocabulary (unique words)\n",
    "vocab = sorted(set(tokens))\n",
    "', '.join(vocab)\n",
    "\n",
    "# count of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "# size of vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# one-hot vector representation\n",
    "# -- empty np array\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "# -- fill-in values\n",
    "for i, word in enumerate(tokens):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "    \n",
    "# print np array\n",
    "pprint(onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# some embellishments\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "df[df ==0] = ''\n",
    "pprint(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Limitations of one-hot encodings\n",
    "===========================\n",
    "\n",
    "There is no natural notion of similarity for one-hot vectors! (Mannings, 2019)\n",
    "\n",
    "**Example 1:** the vectors associated with 'good' and 'fine' are orthogonal:\n",
    "\n",
    "```\n",
    "good = [0, 0, 1, 0, 0, 0]\n",
    "\n",
    "fine = [0, 0, 0, 0, 1, 0]\n",
    "```\n",
    "\n",
    "**Example 2:** 'greasy spoon' and 'British cafe' express the same category of eatery but the intersection of their one-hot vectors is empty.\n",
    "\n",
    "```\n",
    "greasy spoon = [[0, 1, 0,  0], [0, 0, 1, 0]]\n",
    "\n",
    "British cafe = [[1, 0, 0, 0], [0, 0, 0, 1]]\n",
    "```\n",
    "\n",
    "Shall we try to use WordNet’s list of synonyms to get similarity? Likely as not, a bad idea...WordNet has severe limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Modern NLP: Distributional Hypothesis + DL\n",
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From DH to word vectors\n",
    "=====================\n",
    "\n",
    "According to the Distributional Hypothesis, a focal word’s $\\omega$ meaning is a function of the linguistic context ― i.e., the lexical items in the neighborhood of the focal word.\n",
    "\n",
    "Then, considering (all) the many contexts of $\\omega$ (e.g., regulation) helps to create an accurate vector representation of $\\omega$.\n",
    "\n",
    "Sample sentences containing the word 'regulation':\n",
    "\n",
    "```\n",
    "... to encourage and implement the adoption of common REGULATIONs for all forms of motor sports and series across the ...\n",
    "\n",
    "countries should adhere to the cost-benefit paradigm of REGULATION, forcing bureaucrats to outline all the benefits of ...\n",
    "\n",
    "Agencies create REGULATIONs (also known as \"rules\") under the authority of Congress to help ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Word vectors as dense, real valued vectors\n",
    "===================================\n",
    "\n",
    "Ultimately, by observing and analyzing a same word in multiple context, we aim at building a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n",
    "\n",
    "Below is a portion of the [vector](https://spacy.io/usage/vectors-similarity) associated with the word 'banana'.\n",
    "\n",
    "```\n",
    "array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Overview of the $\\texttt{word2vec}$ algorithm\n",
    "================================\n",
    "\n",
    "$\\texttt{word2vec}$ (Mikolov et al. 2013) is a framework for learning word vectors\n",
    "\n",
    "Idea ― given a corpus of text $D$:\n",
    "\n",
    "+ each word $d$ is associated with a vector \n",
    "+ go through each position $k$ in the text, which has a center word $\\omega$ and context words $\\eta$\n",
    "+ use the similarity of the word vectors for $\\omega$ and $\\eta$ to calculate the probability of $\\eta$ given $\\omega$ (or vice versa)\n",
    "+ keep adjusting the word vectors to maximize this probability\n",
    "\n",
    "Source is Manning 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Next week, we'll focus on $\\texttt{word2vec}$ and word vectors only. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
